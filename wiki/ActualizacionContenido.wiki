#summary Actualización del contenido de la Wikipedia

= Actualización del contenido de la Wikipedia =

La idea es actualizar el ultimo dump estatico disponible que es de junio de 2008. Como alternativa, se evalúa la posibilidad de reactivar la rama de la generación dinámica a partir del xml que se genera cada dos semanas.

Voy a llevar aca un log detallado de las pruebas que voy haciendo y recibiendo comentarios que ayuden a resolver.


= Detalles =

(Borrador de trabajo, apunte de referencia, hay mucho mas todavía)


 #. Bajé el último dump.
 #. Busqué los métodos que usan para exportar e importar los dumps [6], con la idea de replicar exactamente el procedimiento utilizado previamente para la generación del html estático. 

 #. El problema consistía en realizar una importación sin problemas de el ultimo dump xml () a una instalación de mediawiki sobre un lamp, que funcione correctamente, y aparentemente desde alli (con tiempo y paciencia) se podría extraer el html estatico sin problemas.
--------------------------
#. Esto implicaba hacer una instalación de mediawiki adecuada, reconstruir la Wikipedia en español en el servidor de pruebas a partir de el ultimo xml disponible, y generar el dump del html estático a partir de ese mirror local utilizando la misma herramienta que se utilizó para generar los html estáticos previos.
 #. Empecé todo el trabajo en mi notebook, y a pesar de que dispuse el espacio en disco necesario para la operación, al ver que había problemas con los resultados y también cuales eran los tiempos de procesamiento, los tiempos que arrojaban las operaciones se hacian muy largos para pensar en hacer varias pruebas, por lo cual replique la instalación en otra máquina con mas espacio y mas potencia (después leería buscando con google que se había tardado 40 dias para hacer el ultimo dump de la version en inglés, y encontrar algunos reportes de bug). 
 #. Instalé una tercera para poder hacer pruebas en paralelo. Como el objetivo era obtener el archivo de resultado una vez, no se trabajó inicialmente en definir un procedimiento determinado para hacerlo, confiando en que la documentación permitiría reproducir un procedimiento bien definido para realizar el procedimiento.
  #. Sin embargo los problemas en los resultados obligaron a reproducir el proceso documentando el detalle de la configuración de las pruebas realizadas a fin de localizar el problema y encontrar o desarrollar una solución. 
 #. Con ese fin configuré un server desde cero, con el home en un disco nuevo de 1 terabyte, para olvidarme de los problemas de espacio, con el reciente Ubuntu 10.04 en 64 bits para mi viejo sempron 1150 con 4 g de ram. (Una vez aislados los problemas de configuración, podía replicar los procesos en la otra maquina con 2 athlon si necesitaba mas procesadores)
 #. Hice todas las configuraciones necesarias con los valores por default.
 #. Rastree posibles configuraciones alternativas publicadas.
 #. Cambié la configuración requerida por mediawiki según (referenciar)
 #. Descarté importDUMP.php según [6] dado el tamaño del trabajo (igual hice una prueba)
 #. Probé con importDump.php tomando en cuenta [9]
 #. Probé con mwdumper. Fuciono la importacion y la exportacion sin reportar errores. Al revisar el resultado aparecian paginas mal renderizadas o corruptas, que rastreando, ya estaban asi en la mediawiki local que se uso para el trabajo. Se verifico con dump reader que esos articulos se veian bien, asi como en la Wikipedia online.

 #. Hice distintas pruebas en distintas maquinas con la misma configuración. El consumo de tiempo y procesamiento es enorme, asi como el espacio en disco que requiere, por lo cual terminé montando un servidor especificamente para esto, sobre un disco nuevo de 1 tera para no tener problemas de espacio.

 #. Encontré que en la documentacion de mwdumper decía que los ultimos dumps habia que procesarlos con la version actual del codigo, no con los binarios disponibles.

 #. Compilé la ultima version de mwdumper, que se supone comipila con gjc, pero daba 89 warnings. Revisando los docs, recomendaban la version de java y recompilé con eso. Ahora me falta hacer la prueba con esta nueva version.  (current)



-------------------------------------------------
La otra alternativa es o bien pasar del xml al html directamente con alguna herramienta, o bien generar el html en forma dinámica. Estoy explorando las alternativas existentes para cada cosa, y tratando de aislar y documentar el problema para tratar de aislarlo.

De [2]:
-------------------------------
NOTE: Dumps are curerntly halted due to Bug 23264
-------------------------------

De [5]:
-------------------------------
What happened to the SQL dumps?

In mid-2005 we upgraded the Wikimedia sites to MediaWiki 1.5, which uses a very different database layout than earlier versions. SQL dumps of the 'cur' and 'old' tables are no longer available because those tables no longer exist.

We don't provide direct dumps of the new 'page', 'revision', and 'text' tables either because aggressive changes to the backend storage make this extra difficult: much data is in fact indirection pointing to another database cluster, and deleted pages which we cannot reproduce may still be present in the raw internal database blobs. The XML dump format provides forward and backward compatibility without requiring authors of third-party dump processing or statistics tools to reproduce our every internal hack. If required, you can use the mwdumper tool (see below) to produce SQL statements compatible with the version 1.4 schema from an XML dump.

De [6]:
----------------------------------------------
Using importDump.php, if you have shell access

    Recommended method for general use, but slow for very big data sets. For very large amounts of data, such as a dump of a big Wikipedia, use mwdumper, and import the links tables as separate SQL dumps.

importDump.php is a command line script located in the maintenance folder of your MediaWiki installation. If you have shell access, you can call importdump.php like this:

php importDump.php <dumpfile>

where <dumpfile> is the name of the XML dump file. If the file is compressed and that has a .gz or .bz2 file extension, it is decompressed automatically.

Note Note: to run importDump.php (or any other tool from the maintenance directory), you need to set up your AdminSettings.php file.

Note Note: running importDump.php can take quite a long time. For a large Wikipedia dump with millions of pages, it may take days, even on a fast server. Also note that the information in meta:Help:Import about merging histories, etc. also applies.

After running importDump.php, you may want to run rebuildrecentchanges.php in order to update the content of your Special:Recentchanges page.
-----------------------------------------------------------------------------------


= Links =

* [1] http://en.wikipedia.org/wiki/Wikipedia_database


* [2] http://static.wikipedia.org/  


* [3] https://bugzilla.wikimedia.org/show_bug.cgi?id=23264


[4] http://dumps.wikimedia.org/eswiki/20100331/

[5] http://meta.wikimedia.org/wiki/Data_dumps

[6] http://www.mediawiki.org/wiki/Manual:Importing_XML_dumps

[7] http://www.mediawiki.org/wiki/Manual:System_administration

[8] http://www.mediawiki.org/wiki/Special:Export

[9] http://www.mediawiki.org/wiki/Manual:AdminSettings.php

[10]http://meta.wikimedia.org/wiki/Help:Import

[11] http://www.mediawiki.org/wiki/Installation

[12] http://www.mediawiki.org/wiki/Manual:Installation_guide

[13] http://meta.wikimedia.org/wiki/Requests_for_dumps

[14] http://www.zedwood.com/article/145/cpp-convert-wikipedia-xml-dump-to-mysql-utf8-safe

[15] http://mundo-n900.blogspot.com/2010/04/wikipedia-offline-en-tu-n900.html

[16] http://users.softlab.ece.ntua.gr/~ttsiod/buildWikipediaOffline.html

[17] http://en.wikipedia.org/wiki/Wikipedia_database#Dynamic_HTML_generation_from_a_local_XML_database_dump

[18] https://bugzilla.wikimedia.org/show_bug.cgi?id=18694

[19] http://www.mediawiki.org/wiki/Manual:MWDumper

[20] http://myy.helia.fi/~karte/tero-dump/

[21] http://www.mediawiki.org/wiki/Manual_talk:MWDumper

[22] http://meta.wikimedia.org/wiki/Help:Import

[23] http://www.mediawiki.org/wiki/Manual:AdminSettings.php

_este listado deberia ser en orden alfabético
Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages